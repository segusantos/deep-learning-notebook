{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18ace99e",
   "metadata": {},
   "source": [
    "# **Deep Learning**\n",
    "\n",
    "The term **deep learning** refers to a subset of machine learning techniques that utilize artificial **neural networks** with multiple layers — hence \"deep\" — to model and understand complex patterns in data.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/ai-ml-dl.svg\" height=\"300\" />\n",
    "    <p><em>Deep learning (DL) is a subset of machine learning (ML), which is a subset of artificial intelligence (AI).</em></p>\n",
    "</div>\n",
    "\n",
    "Just as classic machine learning, deep learning algorithms can be classified into three main categories:\n",
    "- **Supervised learning**: the model is trained on a labeled dataset, meaning that each input data point is paired with the correct output. The model learns to map inputs to outputs by minimizing the error between its predictions and the actual labels. Common applications include image classification, speech recognition, and natural language processing.\n",
    "- **Unsupervised learning**: it involves training models on unlabeled data. The goal is to discover hidden patterns or structures within the data without predefined labels. Techniques such as autoencoders and generative adversarial networks (GANs) are often used for tasks like clustering, dimensionality reduction, and data generation.\n",
    "- **Reinforcement learning**: models learn to make decisions by interacting with an environment. The model, often referred to as an agent, receives feedback in the form of rewards or penalties based on its actions. Deep reinforcement learning has been successfully applied in areas such as game playing (e.g., AlphaGo), robotics, and autonomous systems.\n",
    "\n",
    "In particular, we'll motivate deep learning from an **image classification** perspective, which is a classic supervised learning task where the goal is to assign a label to an input image based on its content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258cfb0a",
   "metadata": {},
   "source": [
    "## **Handwriting Recognition**\n",
    "\n",
    "Suppose we've been tasked with building a function that takes a $28 \\times 28 $ grayscale image of a handwritten digit ($0-9$) as input and outputs the corresponding digit label.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/handwritten-digits.png\" height=\"300\" />\n",
    "    <p><em>Handwritten digits</em></p>\n",
    "</div>\n",
    "\n",
    "Let $x \\in \\mathbb{R}^{28 \\times 28}$ be the input image, and $y \\in \\mathbb{Z}_{10}$ be the output label, where $\\mathbb{Z}_{10} = \\{0, 1, \\ldots, 9\\}$. Then, we want to write a function $f$ such that:\n",
    "\n",
    "$$\n",
    "f(x) = y.\n",
    "$$\n",
    "\n",
    "Similarly, from a programming perspective, we want to implement a function that takes an array as input and returns an integer as output:\n",
    "\n",
    "```python\n",
    "def f(x: np.ndarray) -> int:\n",
    "    digit = ...\n",
    "    return digit\n",
    "```\n",
    "\n",
    "As human programmers, we can easily recognize handwritten digits by looking at the images. However, developing an algorithm that can accurately perform this task is no trivial endeavor.\n",
    "\n",
    "Without loss of generality, we consider the universe of all functions $f$ that can map grayscale images to digits, this is, matrices to scalars:\n",
    "\n",
    "$$\n",
    "f: \\mathbb{R}^{28 \\times 28} \\to \\mathbb{Z}_{10},\n",
    "$$\n",
    "\n",
    "so that the function $f^\\ast$ that we are looking for — this is, the one that correctly maps images to the digits they display — is guaranteed to belong to this set.\n",
    "\n",
    "In order to make searching for $f^\\ast$ tractable, we reframe our problem as a **deep supervised learning** task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad895fd1",
   "metadata": {},
   "source": [
    "## **Neural Networks**\n",
    "\n",
    "Since the space of all functions that map images to digits is uncountably infinite, the **deep supervised learning** paradigm reduces the search space of all possible functions $f: \\mathbb{R}^{28 \\times 28} \\to \\mathbb{Z}_{10}$ to a smaller subset of this space specified by a given **neural network** architecture. Then, $f$ defines the family of functions parameterized by a set of weights $\\theta$, according to the chosen architecture:\n",
    "\n",
    "$$\n",
    "f(x; \\theta) = y.\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/nn.png\" height=\"300\" />\n",
    "    <p><em>Feedforward Neural Network (FNN)</em></p>\n",
    "</div>\n",
    "\n",
    "Now that our neural net depends both on the input $x$ and the parameters $\\theta$, we need to find a way of finding the parameters $\\theta^\\ast$ such that $f(x; \\theta^\\ast)$ approximates $f^\\ast(x)$ as closely as possible for all possible inputs $x$, this is, all possible $28 \\times 28$ images of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13be728",
   "metadata": {},
   "source": [
    "### **Dataset**\n",
    "\n",
    "First, since optimizing $\\theta$ over all possible inputs is infeasible, we  rely on a finite **dataset** of $N$ labeled examples:\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N,\n",
    "$$\n",
    "\n",
    "where each $x_i$ is a $28 \\times 28$ grayscale image of a handwritten digit, and $y_i$ is the corresponding label.\n",
    "\n",
    "For our **handwriting recognition** task, we use the public [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/mnist-dataset.png\" height=\"300\" />\n",
    "    <p><em>MNIST dataset samples</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190fdc9",
   "metadata": {},
   "source": [
    "### **Loss Function**\n",
    "\n",
    "Second, we need to define a measure of how well our function $f(x; \\theta)$ approximates the true function $f^\\ast(x)$ within our dataset $\\mathcal{D}$. For that,\n",
    "we use a **loss function** that quantifies the difference between the predicted output $f(x; \\theta)$ and the true label $y$:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(f(x; \\theta), y).\n",
    "$$\n",
    "\n",
    "Since we model our handwriting recognition task as a **regression** problem — we want to predict a continuous output (the digit label) which we round to the nearest integer — we use the **mean squared error (MSE)** loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(f(x; \\theta), y) = [f(x; \\theta) - y]^2.\n",
    "$$\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/loss.png\" height=\"300\" />\n",
    "    <p><em>Loss function illustration</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b9dd49",
   "metadata": {},
   "source": [
    "## **Optimization**\n",
    "\n",
    "Having defined our dataset $\\mathcal{D}$ and loss function $\\mathcal{L}$, we can now formulate the optimization problem that will allow us to find the optimal parameters $\\theta^\\ast$ for our neural network:\n",
    "\n",
    "\\begin{align*}\n",
    "\\theta^\\ast &= \\arg \\min_\\theta \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(f(x_i; \\theta), y_i),\\\\\n",
    "&= \\arg \\min_\\theta g(\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "Therefore, given a fixed **network architecture** and **dataset**, our goal is to optimize the parameters $\\theta$ so that the **loss function** is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d740ae",
   "metadata": {},
   "source": [
    "### **Differentiation**\n",
    "\n",
    "Then, finding the **neural network** $f(x; \\theta^\\ast)$ that best approximates the true function $f^\\ast(x)$ is equivalent to minimizing $g(\\theta)$ which, in mathematical terms, can be achieved by taking its **gradient** with respect to the parameters $\\theta$ and setting it to zero:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta g(\\theta) = 0.\n",
    "$$\n",
    "\n",
    "However, since neural networks are **non-linear** functions with potentially millions of parameters, solving this system of equations analytically is infeasible. We must therefore resort to **numerical optimization** techniques to find an approximate solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7382359",
   "metadata": {},
   "source": [
    "#### **Gradient Descent**\n",
    "\n",
    "One of the most common numerical optimization techniques used in deep learning is **gradient descent**. This iterative algorithm updates the parameters $\\theta$ in the direction of the negative gradient of the loss function, scaled by a learning rate $\\alpha$:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta g(\\theta_t).\n",
    "$$\n",
    "\n",
    "Hence, we now need an algorithm to efficiently compute the gradient of the loss function with respect to the parameters of the neural network at each iteration $t$.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/gradient-descent.png\" height=\"300\" />\n",
    "    <p><em>Gradient descent</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059c377",
   "metadata": {},
   "source": [
    "#### **Backpropagation**\n",
    "\n",
    "First, we need to understand why **symbolic** or **numerical differentation** methods are not suitable for computing the gradient of the loss function in deep learning:\n",
    "- **Symbolic differentiation**: while it can provide exact gradients, it is impractical for deep neural networks because expressions become extremely large and complex, making symbolic manipulation inefficient and memory-intensive.\n",
    "- **Numerical differentiation**: although it is straightforward to implement, it suffers from numerical instability, it is computationally expensive and can introduce significant approximation errors, especially in high-dimensional parameter spaces.\n",
    "\n",
    "Instead, if we leverage the fact that neural networks are composed of a sequence of **layers**, which are differentiable operations, we can use the **backpropagation** algorithm to efficiently compute the gradient of the loss function with respect to the network parameters.\n",
    "\n",
    "**Automatic differentiation** constructs a **computational graph** that allows computing exact gradients efficiently by systematically applying the **chain rule** of calculus to the operations performed in the neural network.\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"assets/computational-graph.png\" height=\"300\" />\n",
    "    <p><em>Computational graph</em></p>\n",
    "</div>\n",
    "\n",
    "In particular, given that **neural nets** are compositions of elementary functions such as sums, products and non-linear activations, we can define the **forward** and **backward** passes of these operations and let the **backpropagation** algorithm compute the gradients automatically by composing the local gradients using the **chain rule** for any arbitrary neural network architecture.\n",
    "\n",
    "In order to illustrate this point, in what follows we manually implement a simple **autodiff engine** with no dependencies and train a simple neural network for the **handwriting recognition** task. In this [notebook's GitHub repo](https://github.com/segusantos/deep-learning-notebook) you can find a more complete implementation with additional primitives and tests that validate its correctness against [PyTorch](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63571d",
   "metadata": {},
   "source": [
    "## **Autodiff Engine**\n",
    "\n",
    "For simplicity, instead of defining a `Tensor` class as in PyTorch, we define a `Scalar` class that represents a scalar value and its gradient. This class will support basic arithmetic operations and will keep track of the computational graph to enable backpropagation. The `_backward` method refers to the local gradient computation for each operation, whereas `_prev` stores references to the parent nodes in the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35caa480",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scalar:\n",
    "    def __init__(self,\n",
    "                 data: int | float,\n",
    "                 _backward: function = lambda: None,\n",
    "                 _prev: list[Scalar] = []) -> None:\n",
    "        self.data: float = float(data)\n",
    "        self.grad: float = 0.0\n",
    "        self._backward: function = _backward\n",
    "        self._prev: list[Scalar] = _prev\n",
    "\n",
    "    def backward(self) -> None:\n",
    "        topological_order = []\n",
    "        visited = set()\n",
    "        def dfs(scalar: Scalar) -> None:\n",
    "            if scalar not in visited:\n",
    "                visited.add(scalar)\n",
    "                for prev in scalar._prev:\n",
    "                    dfs(prev)\n",
    "                topological_order.append(scalar)\n",
    "        dfs(self)\n",
    "        self.grad = 1.0\n",
    "        for scalar in reversed(topological_order):\n",
    "            scalar._backward()\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Scalar(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df45b906",
   "metadata": {},
   "source": [
    "Now, we define a base `Function` abstract class that will serve as a blueprint for all operations in our autodiff engine. Each operation will inherit from this class and implement the `forward` and `backward` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546af0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class Function(ABC):\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def forward(*inputs: Scalar) -> float:\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def backward(*inputs: Scalar, output: Scalar) -> None:\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def apply(cls, *args: int | float | Scalar) -> Scalar:\n",
    "        inputs = [\n",
    "            arg if isinstance(arg, Scalar) else Scalar(arg)\n",
    "            for arg in args\n",
    "        ]\n",
    "        output = Scalar(\n",
    "            cls.forward(*inputs),\n",
    "            _backward=lambda: cls.backward(*inputs, output),\n",
    "            _prev=inputs\n",
    "        )\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7ce90",
   "metadata": {},
   "source": [
    "### **Addition**\n",
    "\n",
    "First, we implement the addition operation as a subclass of `Function`, where the sum of two `Scalar` objects is computed in the `forward` method, and the gradients are propagated in the `backward` method.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{forward}(a, b) &= a + b, \\\\\n",
    "\\text{backward}() &: \\frac{\\partial \\text{output}}{\\partial a} = 1, \\quad \\frac{\\partial \\text{output}}{\\partial b} = 1.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ea98d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Function):\n",
    "    def forward(a: Scalar, b: Scalar) -> float:\n",
    "        return a.data + b.data\n",
    "\n",
    "    def backward(a: Scalar, b: Scalar, output: Scalar) -> None:\n",
    "        a.grad += 1.0 * output.grad\n",
    "        b.grad += 1.0 * output.grad\n",
    "\n",
    "\n",
    "Scalar.__add__ = lambda self, other: Add.apply(self, other)\n",
    "Scalar.__radd__ = lambda self, other: Add.apply(other, self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2741f3",
   "metadata": {},
   "source": [
    "### **Multiplication**\n",
    "\n",
    "Similarly, we implement the multiplication operation and define its `forward` and `backward` methods.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{forward}(a, b) &= a \\times b, \\\\\n",
    "\\text{backward}() &: \\frac{\\partial \\text{output}}{\\partial a} = b, \\quad \\frac{\\partial \\text{output}}{\\partial b} = a.\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2378113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mul(Function):\n",
    "    def forward(a: Scalar, b: Scalar) -> float:\n",
    "        return a.data * b.data\n",
    "\n",
    "    def backward(a: Scalar, b: Scalar, output: Scalar) -> None:\n",
    "        a.grad += b.data * output.grad\n",
    "        b.grad += a.data * output.grad\n",
    "\n",
    "\n",
    "Scalar.__mul__ = lambda self, other: Mul.apply(self, other)\n",
    "Scalar.__rmul__ = lambda self, other: Mul.apply(other, self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164d55f",
   "metadata": {},
   "source": [
    "### **ReLU**\n",
    "\n",
    "Finally, in order to implement a neural network, we need a non-linear activation function. We choose the **ReLU (Rectified Linear Unit)** activation function, which outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{forward}(a) &= \\max(0, a), \\\\\n",
    "\\text{backward}() &: \\frac{\\partial \\text{output}}{\\partial a} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } a > 0, \\\\\n",
    "0 & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6eff561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Function):\n",
    "    def forward(a: Scalar) -> float:\n",
    "        return (a.data > 0) * a.data\n",
    "\n",
    "    def backward(a: Scalar, output: Scalar) -> None:\n",
    "        a.grad += (a.data > 0) * output.grad\n",
    "\n",
    "\n",
    "Scalar.relu = lambda self: ReLU.apply(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2a80f",
   "metadata": {},
   "source": [
    "### **Neural Network**\n",
    "\n",
    "Now that we have implemented our basic autodiff engine with addition, multiplication, and ReLU operations, we can proceed to build a simple **neural network** for the handwriting recognition task using these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f158b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def parameters(self) -> list[Scalar]:\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        for param in self.parameters():\n",
    "            param.grad = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390b91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, nin: int, nonlin: bool = True) -> None:\n",
    "        self.w = [Scalar(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Scalar(0.0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x: list[Scalar]) -> Scalar:\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self) -> list[Scalar]:\n",
    "        return self.w + [self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22612f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(Module):\n",
    "    def __init__(self, nin: int, nout: int, **kwargs) -> None:\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x: list[Scalar]) -> list[Scalar]:\n",
    "        return [neuron(x) for neuron in self.neurons]\n",
    "\n",
    "    def parameters(self) -> list[Scalar]:\n",
    "        params = []\n",
    "        for neuron in self.neurons:\n",
    "            params.extend(neuron.parameters())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e36d1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(Module):\n",
    "    def __init__(self, nin: int, nouts: list[int]) -> None:\n",
    "        sz = [nin] + nouts\n",
    "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i != len(nouts) - 1) for i in range(len(nouts))]\n",
    "    \n",
    "    def __call__(self, x: list[Scalar]) -> list[Scalar]:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self) -> list[Scalar]:\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params.extend(layer.parameters())\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42b3f90",
   "metadata": {},
   "source": [
    "## **Training**\n",
    "\n",
    "Now that we have defined our `MLP` class, we can train a simple neural network on the MNIST dataset using our autodiff engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:491: RuntimeWarning: The global interpreter lock (GIL) has been enabled to load module 'triton._C.libtriton', which has not declared that it can run safely without the GIL. To override this behavior and keep the GIL disabled (at your own risk), run with PYTHON_GIL=0 or -Xgil=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch import LightningModule\n",
    "from lightning.pytorch import Callback\n",
    "\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB and Path.cwd().name == \"content\":\n",
    "    if not Path(\"transformer-notebook\").exists():\n",
    "        !git clone https://github.com/segusantos/deep-learning-notebook.git\n",
    "        !pip install deep-learning-notebook/\n",
    "    %cd deep-learning-notebook\n",
    "\n",
    "seed = 42\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fa6da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce6b71",
   "metadata": {},
   "source": [
    "### **Manual Training Loop**\n",
    "\n",
    "We train the handmade autodiff MLP on a small MNIST subset to keep the example lightweight while still demonstrating gradient-based updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b7b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _vectorize_image(image):\n",
    "    tensor = ToTensor()(image).view(-1)\n",
    "    return [Scalar(float(value)) for value in tensor]\n",
    "\n",
    "def _mse_loss(outputs: list[Scalar], label: int) -> Scalar:\n",
    "    target = [Scalar(1.0 if idx == label else 0.0) for idx in range(len(outputs))]\n",
    "    diffs = [output + (Scalar(-1.0) * t) for output, t in zip(outputs, target)]\n",
    "    squared_errors = [diff * diff for diff in diffs]\n",
    "    return sum(squared_errors, Scalar(0.0))\n",
    "\n",
    "def _manual_step(model: MLP, image, label: int, learning_rate: float) -> tuple[float, int]:\n",
    "    model.zero_grad()\n",
    "    inputs = _vectorize_image(image)\n",
    "    outputs = model(inputs)\n",
    "    scores = [output.data for output in outputs]\n",
    "    loss = _mse_loss(outputs, label)\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data -= learning_rate * param.grad\n",
    "    prediction = max(range(len(scores)), key=scores.__getitem__)\n",
    "    return loss.data, prediction\n",
    "\n",
    "def _manual_predict(model: MLP, image) -> int:\n",
    "    outputs = model(_vectorize_image(image))\n",
    "    scores = [output.data for output in outputs]\n",
    "    return max(range(len(scores)), key=scores.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95014419",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m     epoch_loss += loss_value\n\u001b[32m     13\u001b[39m     correct += \u001b[38;5;28mint\u001b[39m(prediction == label)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m avg_loss = \u001b[43mepoch_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m / \u001b[38;5;28mlen\u001b[39m(train_subset)\n\u001b[32m     15\u001b[39m accuracy = correct / \u001b[38;5;28mlen\u001b[39m(train_subset)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'float' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "manual_model = MLP(28 * 28, [64, 32, 10])\n",
    "learning_rate = 0.05\n",
    "epochs = 5\n",
    "subset_size = 512\n",
    "train_subset = [training_data[i] for i in range(subset_size)]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    for image, label in train_subset:\n",
    "        loss_value, prediction = _manual_step(manual_model, image, label, learning_rate)\n",
    "        epoch_loss += loss_value\n",
    "        correct += int(prediction == label)\n",
    "    avg_loss = epoch_loss / len(train_subset)\n",
    "    accuracy = correct / len(train_subset)\n",
    "    print(f\"epoch {epoch + 1}: loss={avg_loss:.4f} accuracy={accuracy:.3f}\")\n",
    "\n",
    "test_subset = [test_data[i] for i in range(256)]\n",
    "test_correct = 0\n",
    "for image, label in test_subset:\n",
    "    prediction = _manual_predict(manual_model, image)\n",
    "    test_correct += int(prediction == label)\n",
    "test_accuracy = test_correct / len(test_subset)\n",
    "print(f\"manual model test accuracy on {len(test_subset)} samples: {test_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52494749",
   "metadata": {},
   "source": [
    "### **PyTorch Lightning Training Loop**\n",
    "\n",
    "For comparison, we train a small neural network with PyTorch Lightning using dataloaders on manageable MNIST subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ToTensor()\n",
    "train_dataset_pt = datasets.MNIST(root=\"data\", train=True, download=False, transform=transform)\n",
    "val_dataset_pt = datasets.MNIST(root=\"data\", train=False, download=False, transform=transform)\n",
    "train_subset_indices = list(range(4096))\n",
    "val_subset_indices = list(range(1024))\n",
    "train_loader = DataLoader(Subset(train_dataset_pt, train_subset_indices), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(Subset(val_dataset_pt, val_subset_indices), batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279cf96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitMNIST(LightningModule):\n",
    "    def __init__(self, lr: float = 1e-2) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx: int):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx: int) -> None:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        acc = (preds == y).float().mean()\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD(self.parameters(), lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969aa491",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit_model = LitMNIST(lr=1e-2)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=2, mode=\"min\")\n",
    "trainer = Trainer(\n",
    "    max_epochs=3,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=False,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "trainer.fit(lit_model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
