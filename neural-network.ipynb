{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18ace99e",
      "metadata": {
        "id": "18ace99e"
      },
      "source": [
        "# **Deep Learning**\n",
        "\n",
        "The term **deep learning** refers to a subset of machine learning techniques that utilize artificial **neural networks** with multiple layers — hence \"deep\" — to model and understand complex patterns in data.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/segusantos/deep-learning-notebook/blob/master/assets/ai-ml-dl.svg?raw=1\" height=\"300\" />\n",
        "    <p><em>Deep learning (DL) is a subset of machine learning (ML), which is a subset of artificial intelligence (AI).</em></p>\n",
        "</div>\n",
        "\n",
        "Just as classic machine learning, deep learning algorithms can be classified into three main categories:\n",
        "- **Supervised learning**: the model is trained on a labeled dataset, meaning that each input data point is paired with the correct output. The model learns to map inputs to outputs by minimizing the error between its predictions and the actual labels. Common applications include image classification, speech recognition, and natural language processing.\n",
        "- **Unsupervised learning**: it involves training models on unlabeled data. The goal is to discover hidden patterns or structures within the data without predefined labels. Techniques such as autoencoders and generative adversarial networks (GANs) are often used for tasks like clustering, dimensionality reduction, and data generation.\n",
        "- **Reinforcement learning**: models learn to make decisions by interacting with an environment. The model, often referred to as an agent, receives feedback in the form of rewards or penalties based on its actions. Deep reinforcement learning has been successfully applied in areas such as game playing (e.g., AlphaGo), robotics, and autonomous systems.\n",
        "\n",
        "In particular, we'll motivate deep learning from an **image classification** perspective, which is a classic supervised learning task where the goal is to assign a label to an input image based on its content."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "258cfb0a",
      "metadata": {
        "id": "258cfb0a"
      },
      "source": [
        "## **Handwriting Recognition**\n",
        "\n",
        "Suppose we've been tasked with building a function that takes a $28 \\times 28 $ grayscale image of a handwritten digit ($0-9$) as input and outputs the corresponding digit label.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/segusantos/deep-learning-notebook/blob/master/assets/handwritten-digits.png?raw=1\" height=\"300\" />\n",
        "    <p><em>Handwritten digits</em></p>\n",
        "</div>\n",
        "\n",
        "Let $x \\in \\mathbb{R}^{28 \\times 28}$ be the input image, and $y \\in \\mathbb{Z}_{10}$ be the output label, where $\\mathbb{Z}_{10} = \\{0, 1, \\ldots, 9\\}$. Then, we want to write a function $f$ such that:\n",
        "\n",
        "$$\n",
        "f(x) = y.\n",
        "$$\n",
        "\n",
        "Similarly, from a programming perspective, we want to implement a function that takes an array as input and returns an integer as output:\n",
        "\n",
        "```python\n",
        "def f(x: np.ndarray) -> int:\n",
        "    digit = ...\n",
        "    return digit\n",
        "```\n",
        "\n",
        "As human programmers, we can easily recognize handwritten digits by looking at the images. However, developing an algorithm that can accurately perform this task is no trivial endeavor.\n",
        "\n",
        "Without loss of generality, we consider the universe of all functions $f$ that can map grayscale images to digits, this is, matrices to scalars:\n",
        "\n",
        "$$\n",
        "f: \\mathbb{R}^{28 \\times 28} \\to \\mathbb{Z}_{10},\n",
        "$$\n",
        "\n",
        "so that the function $f^\\ast$ that we are looking for — this is, the one that correctly maps images to the digits they display — is guaranteed to belong to this set.\n",
        "\n",
        "In order to make searching for $f^\\ast$ tractable, we reframe our problem as a **deep supervised learning** task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad895fd1",
      "metadata": {
        "id": "ad895fd1"
      },
      "source": [
        "## **Neural Networks**\n",
        "\n",
        "Since the space of all functions that map images to digits is uncountably infinite, the **deep supervised learning** paradigm reduces the search space of all possible functions $f: \\mathbb{R}^{28 \\times 28} \\to \\mathbb{Z}_{10}$ to a smaller subset of this space specified by a given **neural network** architecture. Then, $f$ defines the family of functions parameterized by a set of weights $\\theta$, according to the chosen architecture:\n",
        "\n",
        "$$\n",
        "f(x; \\theta) = y.\n",
        "$$\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/segusantos/deep-learning-notebook/blob/master/assets/nn.png?raw=1\" height=\"300\" />\n",
        "    <p><em>Feedforward Neural Network (FNN)</em></p>\n",
        "</div>\n",
        "\n",
        "Now that our neural net depends both on the input $x$ and the parameters $\\theta$, we need to find a way of finding the parameters $\\theta^\\ast$ such that $f(x; \\theta^\\ast)$ approximates $f^\\ast(x)$ as closely as possible for all possible inputs $x$, this is, all possible $28 \\times 28$ images of handwritten digits.\n",
        "\n",
        "**Universal Approximation Theorem**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e13be728",
      "metadata": {
        "id": "e13be728"
      },
      "source": [
        "### **Dataset**\n",
        "\n",
        "First, since optimizing $\\theta$ over all possible inputs is infeasible, we  rely on a finite **dataset** of $N$ labeled examples:\n",
        "\n",
        "$$\n",
        "\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^N,\n",
        "$$\n",
        "\n",
        "where each $x_i$ is a $28 \\times 28$ grayscale image of a handwritten digit, and $y_i$ is the corresponding label.\n",
        "\n",
        "For our **handwriting recognition** task, we use the public [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/).\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/segusantos/deep-learning-notebook/blob/master/assets/mnist-dataset.png?raw=1\" height=\"300\" />\n",
        "    <p><em>MNIST dataset samples</em></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4190fdc9",
      "metadata": {
        "id": "4190fdc9"
      },
      "source": [
        "### **Loss Function**\n",
        "\n",
        "Second, we need to define a measure of how well our function $f(x; \\theta)$ approximates the true function $f^\\ast(x)$ within our dataset $\\mathcal{D}$. For that,\n",
        "we use a **loss function** that quantifies the difference between the predicted output $f(x; \\theta)$ and the true label $y$:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(f(x; \\theta), y).\n",
        "$$\n",
        "\n",
        "Since we model our handwriting recognition task as a **regression** problem — we want to predict a continuous output (the digit label) which we round to the nearest integer — we use the **mean squared error (MSE)** loss function:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(f(x; \\theta), y) = [f(x; \\theta) - y]^2.\n",
        "$$\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/segusantos/deep-learning-notebook/blob/master/assets/loss.png?raw=1\" height=\"300\" />\n",
        "    <p><em>Loss function illustration</em></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b9dd49",
      "metadata": {
        "id": "93b9dd49"
      },
      "source": [
        "## **Optimization**\n",
        "\n",
        "Having defined our dataset $\\mathcal{D}$ and loss function $\\mathcal{L}$, we can now formulate the optimization problem that will allow us to find the optimal parameters $\\theta^\\ast$ for our neural network:\n",
        "\n",
        "\\begin{align*}\n",
        "\\theta^\\ast &= \\arg \\min_\\theta \\frac{1}{N} \\sum_{i=1}^N \\mathcal{L}(f(x_i; \\theta), y_i),\\\\\n",
        "&= \\arg \\min_\\theta g(\\theta).\n",
        "\\end{align*}\n",
        "\n",
        "Therefore, given a fixed **network architecture** and **dataset**, our goal is to optimize the parameters $\\theta$ so that the **loss function** is minimized."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69d740ae",
      "metadata": {
        "id": "69d740ae"
      },
      "source": [
        "### **Differentiation**\n",
        "\n",
        "Then, finding the **neural network** $f(x; \\theta^\\ast)$ that best approximates the true function $f^\\ast(x)$ is equivalent to minimizing $g(\\theta)$ which, in mathematical terms, can be achieved by taking its **gradient** with respect to the parameters $\\theta$ and setting it to zero:\n",
        "\n",
        "$$\n",
        "\\nabla_\\theta g(\\theta) = 0.\n",
        "$$\n",
        "\n",
        "However, since neural networks are **non-linear** functions with potentially millions of parameters, solving this system of equations analytically is infeasible. We must therefore resort to **numerical optimization** techniques to find an approximate solution."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7382359",
      "metadata": {
        "id": "d7382359"
      },
      "source": [
        "#### **Gradient Descent**\n",
        "\n",
        "One of the most common numerical optimization techniques used in deep learning is **gradient descent**. This iterative algorithm updates the parameters $\\theta$ in the direction of the negative gradient of the loss function, scaled by a learning rate $\\alpha$:\n",
        "\n",
        "$$\n",
        "\\theta_{t+1} = \\theta_t - \\alpha \\nabla_\\theta g(\\theta_t).\n",
        "$$\n",
        "\n",
        "Hence, we now need an algorithm to efficiently compute the gradient of the loss function with respect to the parameters of the neural network at each iteration $t$.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/segusantos/deep-learning-notebook/blob/master/assets/gradient-descent.png?raw=1\" height=\"300\" />\n",
        "    <p><em>Gradient descent</em></p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3059c377",
      "metadata": {
        "id": "3059c377"
      },
      "source": [
        "#### **Backpropagation**\n",
        "\n",
        "First, we need to understand why **symbolic** or **numerical differentation** methods are not suitable for computing the gradient of the loss function in deep learning:\n",
        "- **Symbolic differentiation**: while it can provide exact gradients, it is impractical for deep neural networks because expressions become extremely large and complex, making symbolic manipulation inefficient and memory-intensive.\n",
        "- **Numerical differentiation**: although it is straightforward to implement, it suffers from numerical instability, it is computationally expensive and can introduce significant approximation errors, especially in high-dimensional parameter spaces.\n",
        "\n",
        "Instead, if we leverage the fact that neural networks are composed of a sequence of **layers**, which are differentiable operations, we can use the **backpropagation** algorithm to efficiently compute the gradient of the loss function with respect to the network parameters.\n",
        "\n",
        "**Automatic differentiation** constructs a **computational graph** that allows computing exact gradients efficiently by systematically applying the **chain rule** of calculus to the operations performed in the neural network.\n",
        "\n",
        "<div align=\"center\">\n",
        "    <img src=\"https://github.com/segusantos/deep-learning-notebook/blob/master/assets/computational-graph.png?raw=1\" height=\"300\" />\n",
        "    <p><em>Computational graph</em></p>\n",
        "</div>\n",
        "\n",
        "In particular, given that **neural nets** are compositions of elementary functions such as sums, products and non-linear activations, we can define the **forward** and **backward** passes of these operations and let the **backpropagation** algorithm compute the gradients automatically by composing the local gradients using the **chain rule** for any arbitrary neural network architecture.\n",
        "\n",
        "In order to illustrate this point, in what follows we manually implement a simple **autodiff engine** with no dependencies and train a simple neural network for the **handwriting recognition** task. In this [notebook's GitHub repo](https://github.com/segusantos/deep-learning-notebook) you can find a more complete implementation with additional primitives and tests that validate its correctness against [PyTorch](https://pytorch.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da63571d",
      "metadata": {
        "id": "da63571d"
      },
      "source": [
        "## **Autodiff Engine**\n",
        "\n",
        "For simplicity, instead of defining a `Tensor` class as in PyTorch, we define a `Scalar` class that represents a scalar value and its gradient. This class will support basic arithmetic operations and will keep track of the computational graph to enable backpropagation. The `_backward` method refers to the local gradient computation for each operation, whereas `_prev` stores references to the parent nodes in the computational graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35caa480",
      "metadata": {
        "id": "35caa480"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "\n",
        "class Scalar:\n",
        "    def __init__(self,\n",
        "                 data: int | float,\n",
        "                 _backward: function = lambda: None,\n",
        "                 _prev: list[Scalar] = []) -> None:\n",
        "        self.data: float = float(data)\n",
        "        self.grad: float = 0.0\n",
        "        self._backward: function = _backward\n",
        "        self._prev: list[Scalar] = _prev\n",
        "\n",
        "    def backward(self) -> None:\n",
        "        topological_order = []\n",
        "        visited = set()\n",
        "        def dfs(scalar: Scalar) -> None:\n",
        "            if scalar not in visited:\n",
        "                visited.add(scalar)\n",
        "                for prev in scalar._prev:\n",
        "                    dfs(prev)\n",
        "                topological_order.append(scalar)\n",
        "        dfs(self)\n",
        "        self.grad = 1.0\n",
        "        for scalar in reversed(topological_order):\n",
        "            scalar._backward()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Scalar(data={self.data}, grad={self.grad})\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df45b906",
      "metadata": {
        "id": "df45b906"
      },
      "source": [
        "Now, we define a base `Function` abstract class that will serve as a blueprint for all operations in our autodiff engine. Each operation will inherit from this class and implement the `forward` and `backward` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546af0c9",
      "metadata": {
        "id": "546af0c9"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "\n",
        "class Function(ABC):\n",
        "    @staticmethod\n",
        "    @abstractmethod\n",
        "    def forward(*inputs: Scalar) -> float:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    @abstractmethod\n",
        "    def backward(*inputs: Scalar, output: Scalar) -> None:\n",
        "        pass\n",
        "\n",
        "    @classmethod\n",
        "    def apply(cls, *args: int | float | Scalar) -> Scalar:\n",
        "        inputs = [\n",
        "            arg if isinstance(arg, Scalar) else Scalar(arg)\n",
        "            for arg in args\n",
        "        ]\n",
        "        output = Scalar(\n",
        "            cls.forward(*inputs),\n",
        "            _backward=lambda: cls.backward(*inputs, output),\n",
        "            _prev=inputs\n",
        "        )\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93a7ce90",
      "metadata": {
        "id": "93a7ce90"
      },
      "source": [
        "### **Addition**\n",
        "\n",
        "First, we implement the addition operation as a subclass of `Function`, where the sum of two `Scalar` objects is computed in the `forward` method, and the gradients are propagated in the `backward` method.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{forward}(a, b) &= a + b, \\\\\n",
        "\\text{backward}() &: \\frac{\\partial \\text{output}}{\\partial a} = 1, \\quad \\frac{\\partial \\text{output}}{\\partial b} = 1.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ea98d2",
      "metadata": {
        "id": "f3ea98d2"
      },
      "outputs": [],
      "source": [
        "class Add(Function):\n",
        "    def forward(a: Scalar, b: Scalar) -> float:\n",
        "        return a.data + b.data\n",
        "\n",
        "    def backward(a: Scalar, b: Scalar, output: Scalar) -> None:\n",
        "        a.grad += 1.0 * output.grad\n",
        "        b.grad += 1.0 * output.grad\n",
        "\n",
        "\n",
        "Scalar.__add__ = lambda self, other: Add.apply(self, other)\n",
        "Scalar.__radd__ = lambda self, other: Add.apply(other, self)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb2741f3",
      "metadata": {
        "id": "cb2741f3"
      },
      "source": [
        "### **Multiplication**\n",
        "\n",
        "Similarly, we implement the multiplication operation and define its `forward` and `backward` methods.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{forward}(a, b) &= a \\times b, \\\\\n",
        "\\text{backward}() &: \\frac{\\partial \\text{output}}{\\partial a} = b, \\quad \\frac{\\partial \\text{output}}{\\partial b} = a.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2378113e",
      "metadata": {
        "id": "2378113e"
      },
      "outputs": [],
      "source": [
        "class Mul(Function):\n",
        "    def forward(a: Scalar, b: Scalar) -> float:\n",
        "        return a.data * b.data\n",
        "\n",
        "    def backward(a: Scalar, b: Scalar, output: Scalar) -> None:\n",
        "        a.grad += b.data * output.grad\n",
        "        b.grad += a.data * output.grad\n",
        "\n",
        "\n",
        "Scalar.__mul__ = lambda self, other: Mul.apply(self, other)\n",
        "Scalar.__rmul__ = lambda self, other: Mul.apply(other, self)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3164d55f",
      "metadata": {
        "id": "3164d55f"
      },
      "source": [
        "### **ReLU**\n",
        "\n",
        "Finally, in order to implement a neural network, we need a non-linear activation function. We choose the **ReLU (Rectified Linear Unit)** activation function, which outputs the input directly if it is positive; otherwise, it outputs zero.\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{forward}(a) &= \\max(0, a), \\\\\n",
        "\\text{backward}() &: \\frac{\\partial \\text{output}}{\\partial a} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } a > 0, \\\\\n",
        "0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6eff561",
      "metadata": {
        "id": "e6eff561"
      },
      "outputs": [],
      "source": [
        "class ReLU(Function):\n",
        "    def forward(a: Scalar) -> float:\n",
        "        return (a.data > 0) * a.data\n",
        "\n",
        "    def backward(a: Scalar, output: Scalar) -> None:\n",
        "        a.grad += (a.data > 0) * output.grad\n",
        "\n",
        "\n",
        "Scalar.relu = lambda self: ReLU.apply(self)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2be2a80f",
      "metadata": {
        "id": "2be2a80f"
      },
      "source": [
        "### **Neural Network**\n",
        "\n",
        "Now that we have implemented our basic autodiff engine with addition, multiplication, and ReLU operations, we can proceed to build a simple **neural network** for the handwriting recognition task using these components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f158b0f",
      "metadata": {
        "id": "5f158b0f"
      },
      "outputs": [],
      "source": [
        "class Module:\n",
        "    def parameters(self) -> list[Scalar]:\n",
        "        return []\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        for param in self.parameters():\n",
        "            param.grad = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2390b91e",
      "metadata": {
        "id": "2390b91e"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "\n",
        "class Neuron(Module):\n",
        "    def __init__(self, nin: int, nonlin: bool = True) -> None:\n",
        "        self.w = [Scalar(random.uniform(-1, 1)) for _ in range(nin)]\n",
        "        self.b = Scalar(0.0)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x: list[Scalar]) -> Scalar:\n",
        "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
        "        return act.relu() if self.nonlin else act\n",
        "\n",
        "    def parameters(self) -> list[Scalar]:\n",
        "        return self.w + [self.b]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22612f2f",
      "metadata": {
        "id": "22612f2f"
      },
      "outputs": [],
      "source": [
        "class Layer(Module):\n",
        "    def __init__(self, nin: int, nout: int, **kwargs) -> None:\n",
        "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x: list[Scalar]) -> list[Scalar]:\n",
        "        return [neuron(x) for neuron in self.neurons]\n",
        "\n",
        "    def parameters(self) -> list[Scalar]:\n",
        "        params = []\n",
        "        for neuron in self.neurons:\n",
        "            params.extend(neuron.parameters())\n",
        "        return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e36d1860",
      "metadata": {
        "id": "e36d1860"
      },
      "outputs": [],
      "source": [
        "class MLP(Module):\n",
        "    def __init__(self, nin: int, nouts: list[int]) -> None:\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i != len(nouts) - 1) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x: list[Scalar]) -> list[Scalar]:\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self) -> list[Scalar]:\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params.extend(layer.parameters())\n",
        "        return params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285bf391",
      "metadata": {},
      "outputs": [],
      "source": [
        "from lightning.pytorch.callbacks import EarlyStopping\n",
        "from lightning.pytorch import Trainer\n",
        "from lightning.pytorch import LightningModule\n",
        "from lightning.pytorch import Callback\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19dc5dc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccdcddfb",
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba281de5",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LightingModel(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 96),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(96, 48),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(48, 10),\n",
        "            nn.Softmax(1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "\n",
        "        X, y = batch\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = self(X)\n",
        "        loss = nn.CrossEntropyLoss()(pred, y)\n",
        "        self.log('train_loss', loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "\n",
        "        X, y = batch\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = self(X)\n",
        "        loss = nn.CrossEntropyLoss()(pred, y)\n",
        "        self.log('val_loss', loss)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "\n",
        "model = LightingModel().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a6d584",
      "metadata": {},
      "outputs": [],
      "source": [
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_loss',  # Metric to monitor\n",
        "    patience=5,         # Number of epochs with no improvement to wait\n",
        "    mode='min',           # 'min' for loss, 'max' for accuracy, etc.\n",
        "    verbose=True,\n",
        "    min_delta=0.005\n",
        ")\n",
        "\n",
        "# Instantiate the Trainer and pass the EarlyStopping callback\n",
        "trainer = Trainer(\n",
        "    max_epochs=10,\n",
        "    callbacks=[early_stopping_callback]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7a7f5b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title prediction grid function\n",
        "def create_prediction_grid(model, dataset, width, height, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Creates a grid of plots showing predictions of a PyTorch model on a given dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to evaluate.\n",
        "        dataset (torch.utils.data.Dataset): The dataset to use for plotting.  Must\n",
        "            return (image, label) tuples.\n",
        "        width (int): The width of the grid (number of columns).\n",
        "        height (int): The height of the grid (number of rows).\n",
        "        device (str, optional): The device to use for the model and data. Defaults to \"cpu\".\n",
        "\n",
        "    Returns:\n",
        "        matplotlib.figure.Figure: The matplotlib figure containing the grid of plots,\n",
        "            or None if there's an error.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the dataset doesn't return (image, label) tuples, or\n",
        "                    if width or height are not positive.\n",
        "        RuntimeError: If the model fails to make a prediction.\n",
        "    \"\"\"\n",
        "    if not isinstance(width, int) or width <= 0:\n",
        "        raise ValueError(\"Width must be a positive integer.\")\n",
        "    if not isinstance(height, int) or height <= 0:\n",
        "        raise ValueError(\"Height must be a positive integer.\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    fig, axs = plt.subplots(height, width, figsize=(width * 2, height * 2))  # Adjust figure size for better viewing\n",
        "    fig.tight_layout() # Add this to automatically adjust subplot parameters to give specified padding.\n",
        "\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=width * height, shuffle=False)\n",
        "    # Get a single batch to fill the grid.\n",
        "    try:\n",
        "        images, labels = next(iter(data_loader))\n",
        "    except StopIteration:\n",
        "        print(\"Error: Dataset is empty.\")\n",
        "        return None\n",
        "    except TypeError:\n",
        "        raise ValueError(\"Dataset must return (image, label) tuples.\")\n",
        "\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for inference\n",
        "        try:\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Error during model prediction: {e}\")\n",
        "\n",
        "    # Ensure axs is treated as a 2D array even if width or height is 1.\n",
        "    if height == 1:\n",
        "        axs = np.array([axs]) if width > 1 else np.array([[axs]])\n",
        "    if width == 1:\n",
        "        axs = np.array([[ax] for ax in axs])\n",
        "\n",
        "\n",
        "    for i in range(height):\n",
        "        for j in range(width):\n",
        "            index = i * width + j\n",
        "            if index >= len(images):\n",
        "                break # Stop if we've processed all images in the batch.\n",
        "\n",
        "            image = images[index].cpu().numpy()\n",
        "            label = labels[index].cpu().numpy()\n",
        "            prediction = predicted[index].cpu().numpy()\n",
        "\n",
        "            # Reshape the image for display (assuming it's MNIST)\n",
        "            image = image.reshape(28, 28)\n",
        "\n",
        "            ax = axs[i, j]\n",
        "            ax.imshow(image, cmap='gray')  # Display the image\n",
        "            ax.axis('off')  # Turn off axis labels\n",
        "\n",
        "            title_color = 'green' if prediction == label else 'red'\n",
        "            ax.set_title(f\"label: {label} pred: {prediction}\", color=title_color, fontsize=10) # Set font size for title\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a077ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "create_prediction_grid(model, training_data, 10, 10, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1704f648",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all data\n",
        "all_inputs = []\n",
        "all_labels = []\n",
        "\n",
        "for i in range(len(test_data)):\n",
        "    x, y = test_data[i]\n",
        "    all_inputs.append(x)\n",
        "    all_labels.append(y)\n",
        "\n",
        "# Stack input tensors\n",
        "inputs_tensor = torch.stack(all_inputs)  # Shape: [N, ...]\n",
        "labels_tensor = torch.tensor(all_labels)\n",
        "\n",
        "# Send to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "inputs_tensor = inputs_tensor.to(device)\n",
        "\n",
        "# Predict\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs_tensor)\n",
        "    test_preds = outputs.argmax(dim=1).cpu()  # shape: [N]\n",
        "\n",
        "# Get ground-truth\n",
        "test_labels = labels_tensor.cpu()  # shape: [N]\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_labels, test_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep-learning-notebook",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
